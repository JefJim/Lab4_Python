#15. Laboratorio #4 - Miner√≠a de Datos
# Universidad T√©cnica Nacional
# Integrantes del grupo: [Jefry Jim√©nez Rocha, Diego Francisco Uma√±a Salas, Marleny Molina Sobalvarro]

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer

#1. Usar  python
#2 Utilice el conjunto de datos, de su elecci√≥n
# 3. Cargar conjunto de datos desde GitHub
url = "https://raw.githubusercontent.com/JefJim/Lab4_Python/main/Costa%20Rica%20Total%20deceases%202014%20-%202021.csv"
try:
    df = pd.read_csv(url, encoding='latin-1')  # Usamos latin-1 por posibles caracteres especiales
    print("Datos cargados exitosamente desde GitHub")
except Exception as e:
    print(f"Error al cargar datos: {e}")
    # Cargar datos locales en caso de error
    df = pd.read_csv("Costa Rica Total deceases 2014 - 2021.csv", encoding='latin-1')

# Limpieza inicial: eliminar filas totalmente vac√≠as si las hay
df = df.dropna(how='all')

# 4. Visualizar caracter√≠sticas b√°sicas del conjunto de datos
print("\n=== Caracter√≠sticas b√°sicas del dataset ===")
print(f"Dimensiones del dataset: {df.shape}")
print("\nPrimeras 5 filas:")
print(df.head())
print("\nResumen estad√≠stico:")
print(df.describe(include='all'))  # Incluye tambi√©n variables categ√≥ricas
print("\nInformaci√≥n del dataset:")
print(df.info())

# 5. Cambiar nombres de columnas a espa√±ol (ya est√°n en espa√±ol, pero podemos estandarizar)
nombres_espanol = {
    'anotrab': 'anio',
    'mestrab': 'mes',
    'nacionalid': 'nacionalidad',
    'Sexo': 'sexo',
    'estcivil': 'estado_civil',
    'edads': 'edad',
    'edadsrec': 'grupo_edad',
    'provincia': 'provincia',
    'pc': 'distrito_residencia',
    'IU': 'indice_urbanizacion',
    'causamuer': 'codigo_causa_muerte',
    'des_causa': 'descripci√≥n_causa_muerte',
    'autopsia': 'autopsia',
    'asistmed': 'asistencia_medica',
    'instmurio': 'lugar_muerte',
    'provocu': 'provincia_muerte',
    'pcocu': 'distrito_muerte',
    'diadef': 'dia_defuncion',
    'mesdef': 'mes_defuncion',
    'anodef': 'anio_defuncion',
    'ocuparec': 'ultima_ocupacion',
    'nacmadre': 'nacionalidad_madre',
    'provregis': 'provincia_registro',
    'pcregis': 'distrito_registro',
    'diadeclara': 'dia_declaracion',
    'mesdeclara': 'mes_declaracion',
    'anodeclara': 'anio_declaracion',
    'grgruposcb': 'grupo_to17',
    'gruposcb': 'grupo_to63',
}
df = df.rename(columns=nombres_espanol)
print("\nNombres de columnas estandarizados:")
print(df.columns)
df['Total_defunciones'] = 1  # Cada registro representa 1 defunci√≥n
total_defunciones = len(df)
print(f"\nüìå Total de defunciones registradas: {total_defunciones:,}")
# 6. Determinar valores nulos con evidencia gr√°fica
print("\n=== Verificaci√≥n de valores nulos ===")
if df.isnull().sum().sum() == 0:
    print("‚úÖ No se encontraron valores nulos en el dataset")
    # Crear un gr√°fico indicando que no hay nulos
    plt.figure(figsize=(6, 2))
    plt.text(0.5, 0.5, 'No se encontraron valores nulos en el dataset', 
             ha='center', va='center', fontsize=12)
    plt.axis('off')
    plt.title("Estado de valores nulos")
    plt.savefig("valores_nulos.png")
    plt.show()
else:
    print("‚ö†Ô∏è Se encontraron valores nulos:")
    print(df.isnull().sum())
    sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)
    plt.title("Mapa de calor de valores nulos")
    plt.savefig("valores_nulos.png")
    plt.show()


# 7. Identificar valores at√≠picos solo en columnas num√©ricas
print("\nüî¢ Identificaci√≥n de columnas num√©ricas:")
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
print("Columnas num√©ricas encontradas:", numeric_cols)
print("\n=== Valores at√≠picos ===")
if not numeric_cols:
    print("‚ö†Ô∏è No se encontraron columnas num√©ricas en el dataset")
    print("üìå Tipos de datos encontrados:")
    print(df.dtypes)
else:
    # 7.1. An√°lisis de valores at√≠picos
    print("\nüìä An√°lisis de valores at√≠picos:")
    plt.figure(figsize=(12, 6))
    for i, col in enumerate(numeric_cols, 1):
        plt.subplot(1, len(numeric_cols), i)
        sns.boxplot(y=df[col])
        plt.title(f"Boxplot de {col}")
    plt.tight_layout()
    plt.savefig("valores_atipicos.png")
    plt.show()

# 8. Funciones para imputaci√≥n de variables (adaptadas para este dataset)
def imputar_nulos(df):
    """Imputa valores nulos seg√∫n el tipo de columna"""
    # Para columnas num√©ricas
    df['Total_defunciones'] = df['Total_defunciones'].fillna(df['Total_defunciones'].median())
    df['edad'] = df['edad'].fillna(df['edad'].median())
    
    # Para columnas categ√≥ricas
    cat_cols = ['sexo', 'estado_civil', 'provincia', 'distrito_residencia', 'nacionalidad', 'descripci√≥n_causa_muerte']
    for col in cat_cols:
        df[col] = df[col].fillna('Desconocido')
    
    return df

def manejar_atipicos(df):
    """Maneja valores at√≠picos usando el m√©todo IQR"""
    # Solo aplicamos a 'Total_defunciones' y 'Edad'
    for col in ['Total_defunciones', 'edad']:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # Winsorizar (reemplazar con los l√≠mites)
        df[col] = np.where(df[col] < lower_bound, lower_bound, 
                          np.where(df[col] > upper_bound, upper_bound, df[col]))
    
    return df

# Aplicar funciones de imputaci√≥n
df = imputar_nulos(df)
df = manejar_atipicos(df)

# 9. Conversi√≥n de tipos de datos
df['anio'] = df['anio'].astype('int')
df['mes'] = df['mes'].astype('category')  # Mes es categ√≥rico ordinal

# 10. Conversi√≥n de variables categ√≥ricas a num√©ricas (solo las necesarias)
# Para este dataset, podr√≠amos no convertir todas ya que muchas son descriptivas
label_encoder = LabelEncoder()
cols_to_encode = ['sexo', 'estado_civil', 'provincia', 'distrito_residencia', 'nacionalidad']
for col in cols_to_encode:
    df[col+'_encoded'] = label_encoder.fit_transform(df[col])

# 11. Estandarizaci√≥n solo de las columnas num√©ricas continuas
scaler = StandardScaler()
df[['Total_defunciones', 'edad']] = scaler.fit_transform(df[['Total_defunciones', 'edad']])

# 12. Correlaci√≥n de variables (solo num√©ricas)
numeric_cols_for_corr = ['Total_defunciones', 'edad', 'anio'] + [col for col in df.columns if '_encoded' in col]
plt.figure(figsize=(15, 10))
corr_matrix = df[numeric_cols_for_corr].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt=".2f")
plt.title("Matriz de correlaci√≥n")
plt.savefig("correlacion.png")
plt.show()

# 13. Identificar variable dependiente y modelo candidato
# En este caso, 'Total_defunciones' podr√≠a ser la variable dependiente
variable_dependiente = 'Total_defunciones'
print(f"\nVariable dependiente identificada: {variable_dependiente}")

# Dado que 'Total_defunciones' es num√©rica continua, ser√≠a un problema de regresi√≥n
print("Problema de regresi√≥n detectado (predicci√≥n de cantidad de defunciones)")
modelo_recomendado = "Random Forest Regressor"
print(f"Modelo recomendado: {modelo_recomendado} (por su capacidad para manejar m√∫ltiples predictores)")

# 14. Guardar conjunto de datos procesado
def corregir_caracteres(texto):
    if isinstance(texto, str):
        return texto.replace('√É', '√≠')
    return texto

# Aplicar la correcci√≥n a todas las columnas de tipo objeto (strings)
for columna in df.select_dtypes(include=['object']).columns:
    df[columna] = df[columna].apply(corregir_caracteres)

# Verificaci√≥n de resultados
print("\nüîç Verificaci√≥n de correcci√≥n de caracteres:")
# Mostrar algunas filas que conten√≠an el problema (si existen)
filas_con_problema = df.apply(lambda row: row.astype(str).str.contains('√É').any(), axis=1)
if filas_con_problema.any():
    print("Se encontraron y corrigieron caracteres '√É' en las siguientes filas:")
    print(df[filas_con_problema].head())
else:
    print("‚úÖ No se encontraron m√°s caracteres '√É' en el dataset")

# Guardar el dataset corregido
df.to_csv('datos_corregidos.csv', index=False, encoding='utf-8-sig')
print("\nüíæ Dataset con caracteres corregidos guardado como 'datos_corregidos.csv'")
df.to_csv('data_process.csv', index=False)
print("\nDataset procesado guardado como 'data_process.csv'")

# Opcional: Guardar tambi√©n como archivo .ipynb
# Este c√≥digo deber√≠a copiarse a un notebook de Jupyter y guardarse como lab4_IC2025.ipynb